{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5110 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 960 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists('business.json'):\n",
    "    pass\n",
    "else:\n",
    "    !wget https://s3.amazonaws.com/cloudfinalprojectsubmission/business.json\n",
    "\n",
    "if os.path.exists('review.json'):\n",
    "    pass\n",
    "else:\n",
    "    !wget https://s3.amazonaws.com/cloudfinalprojectsubmission/review.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunk_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "usa_business_idx = []\n",
    "business_reader = pd.read_table('business.json', header=None, chunksize=chunk_size)\n",
    "categories = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    for i, chunks in enumerate(business_reader):\n",
    "\n",
    "        start_index = chunk_size * i\n",
    "\n",
    "        for index in range(start_index, start_index + len(chunks)):\n",
    "\n",
    "            data_json = pd.read_json(chunks[0][index], typ='series')\n",
    "            if data_json['city'] in ['Pittsburgh', 'Charlotte', 'Urbana-Champaign',\n",
    "                                     'Phoenix', 'Las Vegas', 'Madison']:\n",
    "                usa_business_idx.append(data_json['business_id'])\n",
    "                categories = categories.append(data_json[['business_id', 'categories']], ignore_index=True)\n",
    "except:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categories = categories.drop_duplicates('business_id')\n",
    "categories = categories.set_index('business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11487, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "review_reader = pd.read_table('review.json', header=None, chunksize=chunk_size)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, chunks in enumerate(review_reader):\n",
    "\n",
    "        start_index = chunk_size * i\n",
    "        for index in range(start_index, start_index + len(chunks)):\n",
    "                data_json = pd.read_json(chunks[0][index], typ='series')\n",
    "                bus_id = data_json['business_id']\n",
    "                if (bus_id in usa_business_idx) and (u'Restaurants' in categories.loc[bus_id].categories):\n",
    "                    df = df.append(data_json[['text', 'stars']], ignore_index=True)\n",
    "                    if data_json['stars'] < 4.:\n",
    "                        df = df.append(data_json[['text', 'stars']], ignore_index=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26227, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svetlana/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.feature_extraction.text import _document_frequency\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras import backend as K\n",
    "#from keras.regularizers import l2, activity_l2\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import Nadam\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.cluster import KMeans\n",
    "#import word2vecReader\n",
    "import word2vec\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from sklearn.ensemble import RandomForestClassifier as RandForest\n",
    "from sklearn.svm import SVC\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Went for breakfast on 6/16/14. We received ver...\n",
       "1        I love their stakes and i come here every week...\n",
       "2        Perfect time with hubby, after work which is 4...\n",
       "3        This location never disappoints!! Food is alwa...\n",
       "4        Dude... it's Denny's. It's excellent drunk foo...\n",
       "5        Dude... it's Denny's. It's excellent drunk foo...\n",
       "6        Excellent food , good service too I have had s...\n",
       "7        Chose Denny's for a very late (1:30pm) Sunday ...\n",
       "8        I like the food at Denny's more than meals ser...\n",
       "9        This is the best Denny's in the area. Friendly...\n",
       "10       Got seated right away and the staff was very p...\n",
       "11       Got seated right away and the staff was very p...\n",
       "12       I'm sorry to give any stars...15 minutes for a...\n",
       "13       I'm sorry to give any stars...15 minutes for a...\n",
       "14       We decided on Sapporo after reading so-so revi...\n",
       "15       I travel a lot and have eaten at a lot of Hiba...\n",
       "16       The food was alright. Not good. Not bad. And c...\n",
       "17       The food was alright. Not good. Not bad. And c...\n",
       "18       Sapporo is my favorite Japanese restaurant, pe...\n",
       "19       Been to many hibachis, this one is very clean,...\n",
       "20       My boyfriend and I are practically Hibachi con...\n",
       "21       Great prices, huge portions and friendly envir...\n",
       "22       I stop by here frequently when I am on my way ...\n",
       "23       I stop by here frequently when I am on my way ...\n",
       "24       This is the McDonald's to end all McDonald's, ...\n",
       "25       My orders have come out bad pretty much every ...\n",
       "26       My orders have come out bad pretty much every ...\n",
       "27       This is the place that is close to my house so...\n",
       "28       This is the place that is close to my house so...\n",
       "29       Great convenient drive thru stop for coffee.  ...\n",
       "                               ...                        \n",
       "26197    Johnny's is still great after all of these yea...\n",
       "26198    This place is overrated. The food is no better...\n",
       "26199    This place is overrated. The food is no better...\n",
       "26200                Best burritos ever!  Love this place!\n",
       "26201    Top notch! Some local ingredients. Good prices...\n",
       "26202    Solid mission burrito spot tucked away in hard...\n",
       "26203    Hidden below the Wells Fargo building but I lo...\n",
       "26204    I'm a firm believer of rating business in the ...\n",
       "26205    Massive portions, free guac/mushrooms and othe...\n",
       "26206    Johnny Burrito is exactly what I want in a bur...\n",
       "26207    I've hit up Johnny Burrito's for lunch a few t...\n",
       "26208    Worth waiting in the line (which does move qui...\n",
       "26209    A fun, unique, friendly place to get great foo...\n",
       "26210    The best burritos in Charlotte. Period.\\n\\nIf ...\n",
       "26211    Great burritos, I don't think I can say anythi...\n",
       "26212    I have worked Uptown off and on for about six ...\n",
       "26213    JB's is a great burrito. The BEST in uptown, a...\n",
       "26214    JB's is a great burrito. The BEST in uptown, a...\n",
       "26215    Best burritos in Charlotte! \\n\\nBy far my favo...\n",
       "26216    Just tried this place for the first time today...\n",
       "26217    OMG Burrito overload!!\\n\\nDelicious food, so m...\n",
       "26218    Johnnys is one of the best places in Charlotte...\n",
       "26219    As a first time customer, I was welcomed warml...\n",
       "26220    Make sure you bring your casheesh :) The tamal...\n",
       "26221    Decent food...but pricey. After being asked to...\n",
       "26222    Decent food...but pricey. After being asked to...\n",
       "26223    I've been here many times and the food is grea...\n",
       "26224    Got the regular size steak burrito with queso,...\n",
       "26225    Still waiting on JB to reach out after I was o...\n",
       "26226    Still waiting on JB to reach out after I was o...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['stars'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_len = 140\n",
    "\n",
    "def lowercase(string):\n",
    "    return string[:max_len].lower()\n",
    "\n",
    "X_train['letters'] = X_train.apply(lowercase).apply(list)\n",
    "X_test['letters'] = X_test.apply(lowercase).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lbin = LabelBinarizer()\n",
    "\n",
    "alphabet = list(string.ascii_lowercase + string.digits + string.punctuation + ' ')\n",
    "alphabet_len = len(alphabet)\n",
    "lbin.fit(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(char_list):\n",
    "    binarized = np.zeros((max_len, alphabet_len))\n",
    "    char = lbin.transform(char_list)\n",
    "    binarized[:char.shape[0], :] = char\n",
    "    binarized = binarized.reshape((1, max_len, alphabet_len))\n",
    "    return binarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, MaxPool1D, Flatten, BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, Convolution1D, Convolution2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.vstack(X_train['letters'].apply(binarize)).astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "\n",
    "X_test = np.vstack(X_test['letters'].apply(binarize)).astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "group_lb = LabelBinarizer()\n",
    "y_train = group_lb.fit_transform(y_train).astype('float32')\n",
    "y_test = group_lb.transform(y_test).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def my_init(shape, dtype=None):\n",
    "    return K.random_normal(shape, stddev=0.05, dtype=dtype)    \n",
    "\n",
    "def conv_model(tweet_len, symbols_num):\n",
    "    \n",
    "    inputs = Input(shape=(tweet_len, symbols_num))\n",
    "    l_conv1 = Convolution1D(filters=512, kernel_size=2, activation='relu', kernel_initializer=my_init)(inputs)\n",
    "    l_pool1 = MaxPool1D(pool_size=3)(l_conv1)\n",
    "    bn1 = BatchNormalization()(l_pool1)\n",
    "   \n",
    "    l_conv2 = Convolution1D(filters=64, kernel_size=2, activation='relu')(bn1)\n",
    "    l_pool2 = MaxPool1D(pool_size=4)(l_conv2)\n",
    "    \n",
    "    #l_conv3 = Convolution1D(filters=128, kernel_size=3, activation='relu')(l_pool2)\n",
    "    #l_pool3 = MaxPool1D(pool_size=4)(l_conv3)\n",
    "\n",
    "\n",
    "\n",
    "    #l_conv3 = Convolution1D(filters=128, kernel_size=3, activation='relu')(l_pool2)\n",
    "    \n",
    "    #l_conv4 = Convolution1D(filters=128, kernel_size=3, activation='relu')(l_conv3)\n",
    "    #l_pool4 = MaxPool1D(pool_size=3)(l_conv4)\n",
    "    \n",
    "    #l_conv5 = Convolution1D(filters=256, kernel_size=3, activation='relu')(l_conv4)\n",
    "    #l_pool5 = MaxPool1D(pool_size=3)(l_conv5)\n",
    "    \n",
    "    \n",
    "    fc_layer_1 = Dense(512, activation='relu')(l_pool2)\n",
    "    drop1_out = Dropout(0.5)(fc_layer_1)    \n",
    "    \n",
    "    flat = Flatten()(drop1_out)\n",
    "    fc_layer_2 = Dense(128, activation='relu')(flat)\n",
    "    drop2_out = Dropout(0.5)(fc_layer_2)\n",
    "    \n",
    "    #fc_layer_3 = Dense(128, activation='relu')(drop2_out)\n",
    "    #drop3_out = Dropout(0.5)(fc_layer_2)\n",
    "    \n",
    "    fc_layer_3 = Dense(128, activation='relu')(drop2_out)\n",
    "    drop3_out = Dropout(0.5)(fc_layer_3)\n",
    "    \n",
    "    out = Dense(5,activation='softmax')(drop3_out)\n",
    "    model = Model(inputs, out)\n",
    "    \n",
    "    #Compilation\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'adam' , metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charecter level CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 140, 69)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 139, 512)          71168     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 46, 512)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 46, 512)           2048      \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 45, 64)            65600     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 11, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 11, 512)           33280     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 11, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5632)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               721024    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 910,277\n",
      "Trainable params: 909,253\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.5978 - acc: 0.2340 - val_loss: 1.5854 - val_acc: 0.2715\n",
      "('epoch: ', 0)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.5754 - acc: 0.2566 - val_loss: 1.5277 - val_acc: 0.3085\n",
      "('epoch: ', 1)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 9s - loss: 1.5007 - acc: 0.3039 - val_loss: 1.4544 - val_acc: 0.3393\n",
      "('epoch: ', 2)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.4361 - acc: 0.3450 - val_loss: 1.4296 - val_acc: 0.3593\n",
      "('epoch: ', 3)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.3806 - acc: 0.3662 - val_loss: 1.3777 - val_acc: 0.3864\n",
      "('epoch: ', 4)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.3271 - acc: 0.4033 - val_loss: 1.3302 - val_acc: 0.4104\n",
      "('epoch: ', 5)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.2702 - acc: 0.4349 - val_loss: 1.3309 - val_acc: 0.4112\n",
      "('epoch: ', 6)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.2183 - acc: 0.4564 - val_loss: 1.3753 - val_acc: 0.3889\n",
      "('epoch: ', 7)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.1627 - acc: 0.4854 - val_loss: 1.2725 - val_acc: 0.4468\n",
      "('epoch: ', 8)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.1034 - acc: 0.5158 - val_loss: 1.2562 - val_acc: 0.4562\n",
      "('epoch: ', 9)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.0569 - acc: 0.5354 - val_loss: 1.2498 - val_acc: 0.4505\n",
      "('epoch: ', 10)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 1.0078 - acc: 0.5616 - val_loss: 1.2245 - val_acc: 0.4835\n",
      "('epoch: ', 11)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.9655 - acc: 0.5875 - val_loss: 1.2383 - val_acc: 0.4921\n",
      "('epoch: ', 12)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.9226 - acc: 0.6035 - val_loss: 1.2602 - val_acc: 0.4916\n",
      "('epoch: ', 13)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.8752 - acc: 0.6251 - val_loss: 1.2028 - val_acc: 0.5173\n",
      "('epoch: ', 14)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.8468 - acc: 0.6412 - val_loss: 1.2467 - val_acc: 0.5129\n",
      "('epoch: ', 15)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.8144 - acc: 0.6503 - val_loss: 1.3280 - val_acc: 0.4901\n",
      "('epoch: ', 16)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.7766 - acc: 0.6733 - val_loss: 1.2054 - val_acc: 0.5395\n",
      "('epoch: ', 17)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.7415 - acc: 0.6841 - val_loss: 1.2246 - val_acc: 0.5463\n",
      "('epoch: ', 18)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.7087 - acc: 0.7023 - val_loss: 1.2105 - val_acc: 0.5557\n",
      "('epoch: ', 19)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.6778 - acc: 0.7110 - val_loss: 1.2527 - val_acc: 0.5470\n",
      "('epoch: ', 20)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.6573 - acc: 0.7224 - val_loss: 1.2384 - val_acc: 0.5619\n",
      "('epoch: ', 21)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.6320 - acc: 0.7345 - val_loss: 1.2573 - val_acc: 0.5695\n",
      "('epoch: ', 22)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.6080 - acc: 0.7507 - val_loss: 1.2760 - val_acc: 0.5724\n",
      "('epoch: ', 23)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.5949 - acc: 0.7568 - val_loss: 1.2579 - val_acc: 0.5776\n",
      "('epoch: ', 24)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.5735 - acc: 0.7624 - val_loss: 1.3345 - val_acc: 0.5701\n",
      "('epoch: ', 25)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.5461 - acc: 0.7757 - val_loss: 1.4350 - val_acc: 0.5603\n",
      "('epoch: ', 26)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.5315 - acc: 0.7815 - val_loss: 1.4472 - val_acc: 0.5687\n",
      "('epoch: ', 27)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.5071 - acc: 0.7910 - val_loss: 1.4234 - val_acc: 0.5771\n",
      "('epoch: ', 28)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.5021 - acc: 0.7927 - val_loss: 1.3913 - val_acc: 0.5817\n",
      "('epoch: ', 29)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.4729 - acc: 0.8061 - val_loss: 1.4769 - val_acc: 0.5875\n",
      "('epoch: ', 30)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.4682 - acc: 0.8137 - val_loss: 1.4611 - val_acc: 0.5879\n",
      "('epoch: ', 31)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.4506 - acc: 0.8161 - val_loss: 1.4855 - val_acc: 0.5836\n",
      "('epoch: ', 32)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.4379 - acc: 0.8244 - val_loss: 1.5525 - val_acc: 0.5637\n",
      "('epoch: ', 33)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.4254 - acc: 0.8297 - val_loss: 1.4453 - val_acc: 0.5933\n",
      "('epoch: ', 34)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.4089 - acc: 0.8399 - val_loss: 1.6263 - val_acc: 0.5630\n",
      "('epoch: ', 35)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.4036 - acc: 0.8407 - val_loss: 1.5115 - val_acc: 0.5869\n",
      "('epoch: ', 36)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3841 - acc: 0.8502 - val_loss: 1.6151 - val_acc: 0.5786\n",
      "('epoch: ', 37)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3835 - acc: 0.8518 - val_loss: 1.5962 - val_acc: 0.5938\n",
      "('epoch: ', 38)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3713 - acc: 0.8581 - val_loss: 1.6257 - val_acc: 0.5923\n",
      "('epoch: ', 39)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3655 - acc: 0.8605 - val_loss: 1.5877 - val_acc: 0.6008\n",
      "('epoch: ', 40)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3480 - acc: 0.8650 - val_loss: 1.6401 - val_acc: 0.5949\n",
      "('epoch: ', 41)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3339 - acc: 0.8701 - val_loss: 1.6256 - val_acc: 0.6110\n",
      "('epoch: ', 42)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3349 - acc: 0.8702 - val_loss: 1.7700 - val_acc: 0.5958\n",
      "('epoch: ', 43)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3170 - acc: 0.8788 - val_loss: 1.8047 - val_acc: 0.5955\n",
      "('epoch: ', 44)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3090 - acc: 0.8825 - val_loss: 1.8644 - val_acc: 0.5936\n",
      "('epoch: ', 45)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.3111 - acc: 0.8845 - val_loss: 1.7302 - val_acc: 0.6007\n",
      "('epoch: ', 46)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.2910 - acc: 0.8896 - val_loss: 1.8226 - val_acc: 0.6057\n",
      "('epoch: ', 47)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.2856 - acc: 0.8950 - val_loss: 1.7200 - val_acc: 0.6046\n",
      "('epoch: ', 48)\n",
      "Train on 17572 samples, validate on 8655 samples\n",
      "Epoch 1/1\n",
      "17572/17572 [==============================] - 8s - loss: 0.2779 - acc: 0.8948 - val_loss: 1.8157 - val_acc: 0.5994\n",
      "('epoch: ', 49)\n"
     ]
    }
   ],
   "source": [
    "model = conv_model(tweet_len=max_len, symbols_num=alphabet_len)\n",
    "model.optimizer.lr.set_value(0.0001)\n",
    "nb_epoch = 50\n",
    "for epoch_num in range(nb_epoch):\n",
    "    fit = model.fit(X_train, y_train, batch_size=30, epochs=1, verbose=1, validation_data=(X_test, y_test),)\n",
    "    res = model.predict(X_test)\n",
    "    print(\"epoch: \", epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-46991fd4be0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_bag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#weights = [np.random.normal(scale=0.05 ,size=(1, max_len, alphabet_len)), np.random.normal(scale=0.05, size=)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malphabet_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_num\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_model' is not defined"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "nb_bag = 5\n",
    "for i in range(nb_bag):\n",
    "    #weights = [np.random.normal(scale=0.05 ,size=(1, max_len, alphabet_len)), np.random.normal(scale=0.05, size=)]    \n",
    "    model = conv_model(tweet_len=max_len, symbols_num=alphabet_len)\n",
    "    nb_epoch = 30\n",
    "    for epoch_num in range(nb_epoch):\n",
    "        fit = model.fit(X_train, y_train, batch_size=256, epochs=1, verbose=1, validation_data=(X_test, y_test),\n",
    "                        class_weight=class_weight)\n",
    "        res = model.predict(X_test)\n",
    "        print(\"epoch: \", epoch_num)\n",
    "        print(f1_score(group_lb.inverse_transform(y_test), np.argmax(res, axis=1), average='macro'))\n",
    "    pred_list.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
